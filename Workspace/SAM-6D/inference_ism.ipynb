{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c912d64-f521-447e-95c3-979f47a53deb",
   "metadata": {},
   "source": [
    "1. Run one of the three following cells for the desired dataset (LM-O, YCBV, TLESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd8e05-6192-4c91-ab91-a9df352d0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LM-O\n",
    "data_dir = '/Workspace/Data/Example/'\n",
    "cad_path = data_dir + 'obj_000005.ply'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878fc0e-2831-41ea-af96-93a1b2776963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YCBV\n",
    "data_dir = '/Workspace/Data/Example_ycbv/'\n",
    "cad_path = data_dir + 'obj_000015.ply'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c00bd-4172-4f06-ac69-33192789d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TLESS\n",
    "data_dir = '/Workspace/Data/Example_tless/'\n",
    "cad_path = data_dir + 'obj_000005.ply'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3361ed-fd37-4164-9e98-64563d40ea8a",
   "metadata": {},
   "source": [
    "2. Run all the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19510c3-5bfe-4eeb-a67e-b900d6ffa28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir + 'outputs'\n",
    "rgb_path = data_dir + 'rgb.png'\n",
    "depth_path = data_dir + 'depth.png'\n",
    "cam_path = data_dir + 'camera.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22638bb-7434-4762-b399-5da7e6c67d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentor_model = 'sam'\n",
    "#segmentor_model = 'fastsam'\n",
    "stability_score_thresh = 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf4922-464f-4110-a4df-a58bf96ce054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import distinctipy\n",
    "import glob\n",
    "import imageio.v2 as imageio\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1937d-c662-45ab-a5a7-76b8e252e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9991d-4c3e-4c8a-ad88-1920ecdb5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from PIL import Image\n",
    "from segment_anything.utils.amg import rle_to_mask\n",
    "from skimage.feature import canny\n",
    "from skimage.morphology import binary_dilation\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5f60f-624f-4da4-8281-a809097349b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/code/SAM-6D/Instance_Segmentation_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3fc05-92d9-4d1f-8e98-2a0004524348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.loss import Similarity\n",
    "from model.utils import Detections, convert_npz_to_json\n",
    "from utils.bbox_utils import CropResizePad\n",
    "from utils.inout import load_json, save_json_bop23\n",
    "from utils.poses.pose_utils import get_obj_poses_from_template_level, load_index_level_in_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba9abd-d4eb-4bd0-b054-af9cb14b7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_rgb_transform = T.Compose(\n",
    "        [\n",
    "            T.Normalize(\n",
    "                mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "                std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad7348-f310-4ad8-ac01-74fea061718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(rgb, detections, save_path=\"tmp.png\"):\n",
    "    img = rgb.copy()\n",
    "    gray = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    colors = distinctipy.get_colors(len(detections))\n",
    "    alpha = 0.33\n",
    "\n",
    "    best_score = 0.\n",
    "    for mask_idx, det in enumerate(detections):\n",
    "        if best_score < det['score']:\n",
    "            best_score = det['score']\n",
    "            best_det = detections[mask_idx]\n",
    "\n",
    "    mask = rle_to_mask(best_det[\"segmentation\"])\n",
    "    edge = canny(mask)\n",
    "    edge = binary_dilation(edge, np.ones((2, 2)))\n",
    "    obj_id = best_det[\"category_id\"]\n",
    "    temp_id = obj_id - 1\n",
    "\n",
    "    r = int(255*colors[temp_id][0])\n",
    "    g = int(255*colors[temp_id][1])\n",
    "    b = int(255*colors[temp_id][2])\n",
    "    img[mask, 0] = alpha*r + (1 - alpha)*img[mask, 0]\n",
    "    img[mask, 1] = alpha*g + (1 - alpha)*img[mask, 1]\n",
    "    img[mask, 2] = alpha*b + (1 - alpha)*img[mask, 2]   \n",
    "    img[edge, :] = 255\n",
    "    \n",
    "    img = Image.fromarray(np.uint8(img))\n",
    "    img.save(save_path)\n",
    "    prediction = Image.open(save_path)\n",
    "    \n",
    "    # concat side by side in PIL\n",
    "    img = np.array(img)\n",
    "    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))\n",
    "    concat.paste(rgb, (0, 0))\n",
    "    concat.paste(prediction, (img.shape[1], 0))\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd7ff5-9d11-42d8-b384-5d63569f4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_input_data(depth_path, cam_path, device):\n",
    "    batch = {}\n",
    "    cam_info = load_json(cam_path)\n",
    "    depth = np.array(imageio.imread(depth_path)).astype(np.int32)\n",
    "    cam_K = np.array(cam_info['cam_K']).reshape((3, 3))\n",
    "    depth_scale = np.array(cam_info['depth_scale'])\n",
    "\n",
    "    batch[\"depth\"] = torch.from_numpy(depth).unsqueeze(0).to(device)\n",
    "    batch[\"cam_intrinsic\"] = torch.from_numpy(cam_K).unsqueeze(0).to(device)\n",
    "    batch['depth_scale'] = torch.from_numpy(depth_scale).unsqueeze(0).to(device)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cef49-c2ea-4b30-ab4c-fca07fe94f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{output_dir}/sam6d_results\", exist_ok=True)\n",
    "os.chdir('/code/SAM-6D/Instance_Segmentation_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3f114-6169-4e6d-ab74-4607b427e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../code/SAM-6D/Instance_Segmentation_Model/configs\"):\n",
    "    cfg = compose(config_name='run_inference.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e6597-01b4-45c6-84c4-ff6f7909cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if segmentor_model == \"sam\":\n",
    "    with initialize(version_base=None, config_path=\"../code/SAM-6D/Instance_Segmentation_Model/configs/model\"):\n",
    "        cfg.model = compose(config_name='ISM_sam.yaml')\n",
    "    cfg.model.segmentor_model.stability_score_thresh = stability_score_thresh\n",
    "elif segmentor_model == \"fastsam\":\n",
    "    with initialize(version_base=None, config_path=\"../code/SAM-6D/Instance_Segmentation_Model/configs/model\"):\n",
    "        cfg.model = compose(config_name='ISM_fastsam.yaml')\n",
    "else:\n",
    "    raise ValueError(\"The segmentor_model {} is not supported now!\".format(segmentor_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42d661-2823-4fe9-b0fc-748a4da20993",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Initializing model\")\n",
    "model = instantiate(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f221623-8b71-4f96-b0ef-921c1f320233",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.descriptor_model.model = model.descriptor_model.model.to(device)\n",
    "model.descriptor_model.model.device = device\n",
    "# if there is predictor in the model, move it to device\n",
    "if hasattr(model.segmentor_model, \"predictor\"):\n",
    "    model.segmentor_model.predictor.model = (\n",
    "        model.segmentor_model.predictor.model.to(device)\n",
    "    )\n",
    "else:\n",
    "    model.segmentor_model.model.setup_model(device=device, verbose=True)\n",
    "logging.info(f\"Moving models to {device} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273c687-9e3f-4638-a4c5-dbdfea493601",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Initializing template\")\n",
    "template_dir = os.path.join(output_dir, 'templates')\n",
    "num_templates = len(glob.glob(f\"{template_dir}/*.npy\"))\n",
    "boxes, masks, templates = [], [], []\n",
    "for idx in range(num_templates):\n",
    "    image = Image.open(os.path.join(template_dir, 'rgb_'+str(idx)+'.png'))\n",
    "    mask = Image.open(os.path.join(template_dir, 'mask_'+str(idx)+'.png'))\n",
    "    boxes.append(mask.getbbox())\n",
    "\n",
    "    image = torch.from_numpy(np.array(image.convert(\"RGB\")) / 255).float()\n",
    "    mask = torch.from_numpy(np.array(mask.convert(\"L\")) / 255).float()\n",
    "    image = image * mask[:, :, None]\n",
    "    templates.append(image)\n",
    "    masks.append(mask.unsqueeze(-1))\n",
    "    \n",
    "templates = torch.stack(templates).permute(0, 3, 1, 2)\n",
    "masks = torch.stack(masks).permute(0, 3, 1, 2)\n",
    "boxes = torch.tensor(np.array(boxes))\n",
    "\n",
    "processing_config = OmegaConf.create(\n",
    "    {\n",
    "        \"image_size\": 224,\n",
    "    }\n",
    ")\n",
    "proposal_processor = CropResizePad(processing_config.image_size)\n",
    "templates = proposal_processor(images=templates, boxes=boxes).to(device)\n",
    "masks_cropped = proposal_processor(images=masks, boxes=boxes).to(device)\n",
    "\n",
    "model.ref_data = {}\n",
    "model.ref_data[\"descriptors\"] = model.descriptor_model.compute_features(\n",
    "                templates, token_name=\"x_norm_clstoken\"\n",
    "            ).unsqueeze(0).data\n",
    "model.ref_data[\"appe_descriptors\"] = model.descriptor_model.compute_masked_patch_feature(\n",
    "                templates, masks_cropped[:, 0, :, :]\n",
    "            ).unsqueeze(0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd229c-6add-44fa-bbe2-5e9728596726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "detections = model.segmentor_model.generate_masks(np.array(rgb))\n",
    "detections = Detections(detections)\n",
    "query_decriptors, query_appe_descriptors = model.descriptor_model.forward(np.array(rgb), detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3caa545-eb27-4ad0-ad75-8e4889b2f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching descriptors\n",
    "(\n",
    "    idx_selected_proposals,\n",
    "    pred_idx_objects,\n",
    "    semantic_score,\n",
    "    best_template,\n",
    ") = model.compute_semantic_score(query_decriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1751052-3387-4843-a84a-ff8ad01ee08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update detections\n",
    "detections.filter(idx_selected_proposals)\n",
    "query_appe_descriptors = query_appe_descriptors[idx_selected_proposals, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ab3c2-942d-407c-b2ff-a0df1cac5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the appearance score\n",
    "appe_scores, ref_aux_descriptor= model.compute_appearance_score(best_template, pred_idx_objects, query_appe_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d7ead-72f3-47a9-840f-3852204d21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the geometric score\n",
    "batch = batch_input_data(depth_path, cam_path, device)\n",
    "template_poses = get_obj_poses_from_template_level(level=2, pose_distribution=\"all\")\n",
    "template_poses[:, :3, 3] *= 0.4\n",
    "poses = torch.tensor(template_poses).to(torch.float32).to(device)\n",
    "model.ref_data[\"poses\"] =  poses[load_index_level_in_level2(0, \"all\"), :, :]\n",
    "\n",
    "mesh = trimesh.load_mesh(cad_path)\n",
    "model_points = mesh.sample(2048).astype(np.float32) / 1000.0\n",
    "model.ref_data[\"pointcloud\"] = torch.tensor(model_points).unsqueeze(0).data.to(device)\n",
    "\n",
    "image_uv = model.project_template_to_image(best_template, pred_idx_objects, batch, detections.masks)\n",
    "\n",
    "geometric_score, visible_ratio = model.compute_geometric_score(\n",
    "    image_uv, detections, query_appe_descriptors, ref_aux_descriptor, visible_thred=model.visible_thred\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7691a46-cbaf-4299-a170-baa935e3a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final score\n",
    "final_score = (semantic_score + appe_scores + geometric_score*visible_ratio) / (1 + 1 + visible_ratio)\n",
    "detections.add_attribute(\"scores\", final_score)\n",
    "detections.add_attribute(\"object_ids\", torch.zeros_like(final_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc0a01-3cc6-4ad0-9971-f00f55d55e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best detection\n",
    "max_score = max(detections.scores)\n",
    "keep_idxs = detections.scores == max_score\n",
    "for key in detections.keys:\n",
    "    setattr(detections, key, getattr(detections, key)[keep_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bdc6a-139f-4db8-9782-b4a2998ccc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections.to_numpy()\n",
    "save_path = f\"{output_dir}/sam6d_results/detection_ism\"\n",
    "detections.save_to_file(0, 0, 0, save_path, \"Custom\", return_results=False)\n",
    "detections = convert_npz_to_json(idx=0, list_npz_paths=[save_path+\".npz\"])\n",
    "save_json_bop23(save_path+\".json\", detections)\n",
    "vis_img = visualize(rgb, detections, f\"{output_dir}/sam6d_results/vis_ism.png\")\n",
    "vis_img.save(f\"{output_dir}/sam6d_results/vis_ism.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c882e-5d13-42fd-953f-debc3e4a3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vis_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Segmentation mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2b81a-07a0-4838-9a5f-02ce6333d871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
